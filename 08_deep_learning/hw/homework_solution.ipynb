{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "84d75784",
            "metadata": {},
            "source": [
                "## Setup: Random Seeds"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "9e9ce23b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "PyTorch version: 2.8.0+cu128\n",
                        "Device: cuda\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "\n",
                "SEED = 42\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.manual_seed(SEED)\n",
                "    torch.cuda.manual_seed_all(SEED)\n",
                "\n",
                "torch.backends.cudnn.deterministic = True\n",
                "torch.backends.cudnn.benchmark = False\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "042de0de",
            "metadata": {},
            "source": [
                "## Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "ee205c40",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms\n",
                "from PIL import Image\n",
                "import os"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0ea0f8ec",
            "metadata": {},
            "source": [
                "---\n",
                "# QUESTION 1: Which loss function to use?\n",
                "\n",
                "**Options:**\n",
                "- `nn.MSELoss()`\n",
                "- `nn.BCEWithLogitsLoss()`\n",
                "- `nn.CrossEntropyLoss()`\n",
                "- `nn.CosineEmbeddingLoss()`\n",
                "\n",
                "** Answer:** `nn.BCEWithLogitsLoss()` BCEWithLogitsLoss (Binary Cross Entropy with Logits)\n",
                "\n",
                "Used for binary classification (2 classes)\n",
                "Expects 1 output neuron\n",
                "Applies sigmoid internally"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "c355f537",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model created successfully\n",
                        "HairTypesCNN(\n",
                        "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
                        "  (relu1): ReLU()\n",
                        "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
                        "  (fc1): Linear(in_features=313632, out_features=64, bias=True)\n",
                        "  (relu2): ReLU()\n",
                        "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "class HairTypesCNN(nn.Module):\n",
                "    def __init__(self):\n",
                "        super(HairTypesCNN, self).__init__()\n",
                "        \n",
                "        # Input: (3, 200, 200)\n",
                "        # Conv2d: 32 filters, kernel 3x3, padding=0, stride=1\n",
                "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3, 3), padding=0, stride=1)\n",
                "        self.relu1 = nn.ReLU()\n",
                "        \n",
                "        # MaxPool2d: 2x2\n",
                "        self.maxpool = nn.MaxPool2d(kernel_size=(2, 2))\n",
                "        \n",
                "        # After Conv2d(3x3, no padding): (200-3+1) = 198 -> (32, 198, 198)\n",
                "        # After MaxPool2d(2x2): 198/2 = 99 -> (32, 99, 99)\n",
                "        # Flattened: 32 * 99 * 99 = 313632\n",
                "        self.flatten_size = 32 * 99 * 99\n",
                "        \n",
                "        # Linear layer: 64 neurons with ReLU\n",
                "        self.fc1 = nn.Linear(self.flatten_size, 64)\n",
                "        self.relu2 = nn.ReLU()\n",
                "        \n",
                "        # Output layer: 1 neuron for binary classification\n",
                "        self.fc2 = nn.Linear(64, 1)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.conv1(x)\n",
                "        x = self.relu1(x)\n",
                "        x = self.maxpool(x)\n",
                "        x = x.view(-1, self.flatten_size)\n",
                "        x = self.fc1(x)\n",
                "        x = self.relu2(x)\n",
                "        x = self.fc2(x)\n",
                "        return x\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "model = HairTypesCNN().to(device)\n",
                "print(\"Model created successfully\")\n",
                "print(model)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q2_md",
            "metadata": {},
            "source": [
                "---\n",
                "# QUESTION 2: Number of parameters\n",
                "\n",
                "What's the total number of parameters of the model?\n",
                "\n",
                "**Answer:** `20073473`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "a808f9a3",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "----------------------------------------------------------------\n",
                        "        Layer (type)               Output Shape         Param #\n",
                        "================================================================\n",
                        "            Conv2d-1         [-1, 32, 198, 198]             896\n",
                        "              ReLU-2         [-1, 32, 198, 198]               0\n",
                        "         MaxPool2d-3           [-1, 32, 99, 99]               0\n",
                        "            Linear-4                   [-1, 64]      20,072,512\n",
                        "              ReLU-5                   [-1, 64]               0\n",
                        "            Linear-6                    [-1, 1]              65\n",
                        "================================================================\n",
                        "Total params: 20,073,473\n",
                        "Trainable params: 20,073,473\n",
                        "Non-trainable params: 0\n",
                        "----------------------------------------------------------------\n",
                        "Input size (MB): 0.46\n",
                        "Forward/backward pass size (MB): 21.54\n",
                        "Params size (MB): 76.57\n",
                        "Estimated Total Size (MB): 98.57\n",
                        "----------------------------------------------------------------\n"
                    ]
                }
            ],
            "source": [
                "# Option 1: Using torchsummary (uncomment if installed)\n",
                "from torchsummary import summary\n",
                "summary(model, (3, 200, 200))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "q2_code",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total parameters: 20073473\n"
                    ]
                }
            ],
            "source": [
                "# Option 2: Manual counting\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "print(f\"Total parameters: {total_params}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q3_setup_md",
            "metadata": {},
            "source": [
                "## Data Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "q3_data_prep",
            "metadata": {},
            "outputs": [],
            "source": [
                "from torchvision.datasets import ImageFolder\n",
                "\n",
                "train_transforms = transforms.Compose([\n",
                "    transforms.Resize((200, 200)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(\n",
                "        mean=[0.485, 0.456, 0.406],\n",
                "        std=[0.229, 0.224, 0.225]\n",
                "    ) # ImageNet normalization\n",
                "])\n",
                "\n",
                "test_transforms = transforms.Compose([\n",
                "    transforms.Resize((200, 200)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(\n",
                "        mean=[0.485, 0.456, 0.406],\n",
                "        std=[0.229, 0.224, 0.225]\n",
                "    )\n",
                "])\n",
                "\n",
                "train_dataset = ImageFolder('./data/train', transform=train_transforms)\n",
                "validation_dataset = ImageFolder('./data/test', transform=test_transforms)\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, num_workers=2)\n",
                "validation_loader = DataLoader(validation_dataset, batch_size=20, shuffle=False, num_workers=2)\n",
                "\n",
                "class_names = train_dataset.classes\n",
                "print(f\"Class names: {class_names}\")\n",
                "print(f\"Train size: {len(train_dataset)}\")\n",
                "print(f\"Validation size: {len(validation_dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q3_train_md",
            "metadata": {},
            "source": [
                "## Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "q3_train_loop",
            "metadata": {},
            "outputs": [],
            "source": [
                "criterion = nn.BCEWithLogitsLoss()\n",
                "optimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.8)\n",
                "\n",
                "num_epochs = 10\n",
                "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
                "\n",
                "for epoch in range(num_epochs):\n",
                "    model.train()\n",
                "    running_loss = 0.0\n",
                "    correct_train = 0\n",
                "    total_train = 0\n",
                "    for images, labels in train_loader:\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)\n",
                "\n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        running_loss += loss.item() * images.size(0)\n",
                "        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy\n",
                "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
                "        total_train += labels.size(0)\n",
                "        correct_train += (predicted == labels).sum().item()\n",
                "\n",
                "    epoch_loss = running_loss / len(train_dataset)\n",
                "    epoch_acc = correct_train / total_train\n",
                "    history['loss'].append(epoch_loss)\n",
                "    history['acc'].append(epoch_acc)\n",
                "\n",
                "    model.eval()\n",
                "    val_running_loss = 0.0\n",
                "    correct_val = 0\n",
                "    total_val = 0\n",
                "    with torch.no_grad():\n",
                "        for images, labels in validation_loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            labels = labels.float().unsqueeze(1)\n",
                "\n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs, labels)\n",
                "\n",
                "            val_running_loss += loss.item() * images.size(0)\n",
                "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
                "            total_val += labels.size(0)\n",
                "            correct_val += (predicted == labels).sum().item()\n",
                "\n",
                "    val_epoch_loss = val_running_loss / len(validation_dataset)\n",
                "    val_epoch_acc = correct_val / total_val\n",
                "    history['val_loss'].append(val_epoch_loss)\n",
                "    history['val_acc'].append(val_epoch_acc)\n",
                "\n",
                "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
                "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
                "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q3_ans_md",
            "metadata": {},
            "source": [
                "---\n",
                "# QUESTION 3: Median of training accuracy\n",
                "\n",
                "What is the median of training accuracy for all the epochs for this model?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "q3_ans_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "median_acc = np.median(history['acc'])\n",
                "print(f\"Median Training Accuracy: {median_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q4_ans_md",
            "metadata": {},
            "source": [
                "---\n",
                "# QUESTION 4: Standard deviation of training loss\n",
                "\n",
                "What is the standard deviation of training loss for all the epochs for this model?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "q4_ans_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "std_loss = np.std(history['loss'])\n",
                "print(f\"Standard Deviation of Training Loss: {std_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q5_setup_md",
            "metadata": {},
            "source": [
                "## Data Augmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "q5_data_aug",
            "metadata": {},
            "outputs": [],
            "source": [
                "train_transforms = transforms.Compose([\n",
                "    transforms.Resize((200, 200)),\n",
                "    transforms.RandomRotation(50),\n",
                "    transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(\n",
                "        mean=[0.485, 0.456, 0.406],\n",
                "        std=[0.229, 0.224, 0.225]\n",
                "    )\n",
                "])\n",
                "\n",
                "train_dataset = ImageFolder('./data/train', transform=train_transforms)\n",
                "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, num_workers=2)\n",
                "\n",
                "print(\"Data augmentation applied to training set.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q5_train_md",
            "metadata": {},
            "source": [
                "## Continue Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "q5_train_loop",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Continue training for 10 more epochs\n",
                "num_epochs = 10\n",
                "# We will store the history for these new epochs separately to answer Q5 and Q6 easily\n",
                "history_aug = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
                "\n",
                "for epoch in range(num_epochs):\n",
                "    model.train()\n",
                "    running_loss = 0.0\n",
                "    correct_train = 0\n",
                "    total_train = 0\n",
                "    for images, labels in train_loader:\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        labels = labels.float().unsqueeze(1)\n",
                "\n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        running_loss += loss.item() * images.size(0)\n",
                "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
                "        total_train += labels.size(0)\n",
                "        correct_train += (predicted == labels).sum().item()\n",
                "\n",
                "    epoch_loss = running_loss / len(train_dataset)\n",
                "    epoch_acc = correct_train / total_train\n",
                "    history_aug['loss'].append(epoch_loss)\n",
                "    history_aug['acc'].append(epoch_acc)\n",
                "\n",
                "    model.eval()\n",
                "    val_running_loss = 0.0\n",
                "    correct_val = 0\n",
                "    total_val = 0\n",
                "    with torch.no_grad():\n",
                "        for images, labels in validation_loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            labels = labels.float().unsqueeze(1)\n",
                "\n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs, labels)\n",
                "\n",
                "            val_running_loss += loss.item() * images.size(0)\n",
                "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
                "            total_val += labels.size(0)\n",
                "            correct_val += (predicted == labels).sum().item()\n",
                "\n",
                "    val_epoch_loss = val_running_loss / len(validation_dataset)\n",
                "    val_epoch_acc = correct_val / total_val\n",
                "    history_aug['val_loss'].append(val_epoch_loss)\n",
                "    history_aug['val_acc'].append(val_epoch_acc)\n",
                "\n",
                "    print(f\"Epoch {epoch+11}/{num_epochs+10}, \"\n",
                "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
                "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q5_ans_md",
            "metadata": {},
            "source": [
                "---\n",
                "# QUESTION 5: Mean of test loss\n",
                "\n",
                "What is the mean of test loss for all the epochs for the model trained with augmentations?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "q5_ans_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "mean_val_loss_aug = np.mean(history_aug['val_loss'])\n",
                "print(f\"Mean Validation Loss (Augmented): {mean_val_loss_aug:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q6_ans_md",
            "metadata": {},
            "source": [
                "---\n",
                "# QUESTION 6: Average of test accuracy for the last 5 epochs\n",
                "\n",
                "What's the average of test accuracy for the last 5 epochs (from 6 to 10) for the model trained with augmentations?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "q6_ans_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "avg_val_acc_last5 = np.mean(history_aug['val_acc'][-5:])\n",
                "print(f\"Average Validation Accuracy (Last 5 epochs): {avg_val_acc_last5:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "next_cell",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "pytorch_clean",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}